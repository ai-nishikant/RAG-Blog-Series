# -----------------------------------------
# API Keys (Optional — only needed for LLM-based demos)
# -----------------------------------------

# OpenAI key (if using GPT models in controller or patterns)
OPENAI_API_KEY=your_openai_key_here

# Groq API key (if using Llama 3.1 or Mixtral for evaluation experiments)
GROQ_API_KEY=your_groq_key_here

# Anthropic API key (if needed for canary prompts or reasoning audits)
ANTHROPIC_API_KEY=your_anthropic_key_here


# -----------------------------------------
# Evaluation Config
# -----------------------------------------

# Path overrides (optional: repo defaults will be used if these are empty)
EVAL_CONFIG=./config/evaluation_config.yaml
SCENARIO_CONFIG=./config/demo_scenarios.yaml

# Logging directory (for writing evaluation traces)
EVAL_LOG_DIR=./data/logs

# Dashboard output directory — optional (for JSON payloads)
DASHBOARD_PAYLOAD_DIR=./dashboards/examples


# -----------------------------------------
# Environment Flags
# -----------------------------------------

# Set to "1" to enable verbose logging during evaluation
DEBUG_EVAL=0

# Set to "1" to enable experimental features (shadow eval, cost audits)
EXPERIMENTAL_MODE=0
