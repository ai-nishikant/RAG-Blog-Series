{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 – Controller Decision Logic\n",
        "\n",
        "This notebook implements Step 3 from the blog:\n",
        "\n",
        "- Use Stage A/B/C results\n",
        "- Decide which corrective action is appropriate\n",
        "- Demonstrate how an evaluation controller can stabilize a RAG system\n",
        "\n",
        "We use:\n",
        "\n",
        "- `rag_eval.stages.evaluate_all_stages`\n",
        "- `rag_eval.controller.EvaluationController`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
        "\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.append(SRC_PATH)\n",
        "\n",
        "from rag_eval.metrics import compute_all_metrics\n",
        "from rag_eval.stages import evaluate_all_stages\n",
        "from rag_eval.controller import EvaluationController\n",
        "\n",
        "PROJECT_ROOT, SRC_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper: Evaluate and Control in One Step\n",
        "\n",
        "We define a small helper function that:\n",
        "\n",
        "1. Computes metrics\n",
        "2. Evaluates Stage A/B/C\n",
        "3. Asks the controller which action to take\n",
        "4. Executes that action (simulated)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "controller = EvaluationController()\n",
        "\n",
        "def run_eval_and_control(query, metadata, retrieved_docs, reference, output, label):\n",
        "    print(f\"\\n=== Scenario: {label} ===\")\n",
        "    metrics = compute_all_metrics(\n",
        "        reference=reference,\n",
        "        output=output,\n",
        "        latency_ms=metadata.get(\"latency_ms\", 120),\n",
        "        token_count=metadata.get(\"token_count\", 80),\n",
        "        retrieval_ms=metadata.get(\"retrieval_ms\", 30),\n",
        "    )\n",
        "\n",
        "    stage_results = evaluate_all_stages(\n",
        "        query=query,\n",
        "        metadata=metadata,\n",
        "        retrieved_docs=retrieved_docs,\n",
        "        gen_metrics={\"intrinsic\": metrics.intrinsic},\n",
        "    )\n",
        "\n",
        "    action_name = controller.choose_action(stage_results)\n",
        "    correction = controller.execute(action_name)\n",
        "\n",
        "    print(\"Stage results:\", stage_results)\n",
        "    print(\"Chosen action:\", action_name)\n",
        "    print(\"Correction detail:\", correction)\n",
        "\n",
        "    return stage_results, correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scenario 1: Everything Is Healthy (No Action)\n",
        "\n",
        "The controller should recommend `noop`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "query = \"What changed in the AI auditing policy in 2024?\"\n",
        "metadata = {\"domain\": \"policy\", \"max_query_tokens\": 50}\n",
        "retrieved_docs = [\n",
        "    {\"id\": \"doc1\", \"content\": \"Policy updated in 2024 to include AI auditing guidelines.\"},\n",
        "    {\"id\": \"doc2\", \"content\": \"Details of AI auditing process introduced in 2024.\"},\n",
        "    {\"id\": \"doc3\", \"content\": \"Background on AI auditing requirements.\"},\n",
        "]\n",
        "reference = \"The policy was updated in 2024 to include new AI auditing guidelines.\"\n",
        "output = \"The policy was updated in 2024 with new guidelines for AI auditing and oversight.\"\n",
        "\n",
        "run_eval_and_control(query, metadata, retrieved_docs, reference, output, label=\"healthy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scenario 2: Retrieval Drift (Stage B Fails → adjust_retrieval)\n",
        "\n",
        "Here we simulate poor retrieval while prompt and query remain reasonable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "retrieved_docs_drifted = [\n",
        "    {\"id\": \"doc1\", \"content\": \"Unrelated content about billing.\"},\n",
        "]\n",
        "\n",
        "output_drifted = \"The policy mentions some updates, but details are missing.\"\n",
        "\n",
        "run_eval_and_control(\n",
        "    query,\n",
        "    metadata,\n",
        "    retrieved_docs_drifted,\n",
        "    reference,\n",
        "    output_drifted,\n",
        "    label=\"retrieval_drift\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scenario 3: Reasoning / Grounding Drift (Stage C Fails → adjust_prompt)\n",
        "\n",
        "Here retrieval is healthy, but the model produces an incorrect or weakly\n",
        "grounded answer, leading to low intrinsic score and a prompt-level fix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "output_bad = \"The policy introduced new marketing guidelines unrelated to AI auditing.\"\n",
        "\n",
        "run_eval_and_control(\n",
        "    query,\n",
        "    metadata,\n",
        "    retrieved_docs,\n",
        "    reference,\n",
        "    output_bad,\n",
        "    label=\"reasoning_drift\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
