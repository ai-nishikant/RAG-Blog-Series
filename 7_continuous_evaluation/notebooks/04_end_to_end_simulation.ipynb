{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 â€“ End-to-End Continuous Evaluation Simulation\n",
        "\n",
        "This notebook combines all components:\n",
        "\n",
        "- metrics (Step 1)\n",
        "- multi-stage evaluation loop (Step 2)\n",
        "- evaluation controller (Step 3)\n",
        "- feedback loops (Step 4)\n",
        "- drift scenarios (Step 5 / scenarios.py)\n",
        "\n",
        "The goal is to simulate how a continuous evaluation pipeline detects and\n",
        "responds to different kinds of drift in a RAG system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
        "CONFIG_PATH = os.path.join(PROJECT_ROOT, \"config\")\n",
        "\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.append(SRC_PATH)\n",
        "\n",
        "from rag_eval.metrics import compute_all_metrics\n",
        "from rag_eval.stages import evaluate_all_stages\n",
        "from rag_eval.controller import EvaluationController\n",
        "from rag_eval.feedback_loops import OfflineFeedbackLoop, OnlineFeedbackLoop\n",
        "from rag_eval.scenarios import load_scenarios\n",
        "\n",
        "PROJECT_ROOT, SRC_PATH, CONFIG_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Drift Scenarios\n",
        "\n",
        "We load synthetic scenarios from `config/demo_scenarios.yaml`.\n",
        "\n",
        "Each scenario describes a different kind of drift:\n",
        "\n",
        "- retrieval quality degradation\n",
        "- reasoning / grounding drift\n",
        "- benign changes (no failure)\n",
        "\n",
        "You can customize these scenarios to reflect your own system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "scenario_file = os.path.join(CONFIG_PATH, \"demo_scenarios.yaml\")\n",
        "scenarios = load_scenarios(scenario_file)\n",
        "\n",
        "[(s.name, s.description) for s in scenarios]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up Controller and Feedback Loops\n",
        "\n",
        "We will:\n",
        "\n",
        "- run each scenario\n",
        "- compute metrics\n",
        "- evaluate stages\n",
        "- ask the controller for an action\n",
        "- record results in offline and online feedback loops\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "controller = EvaluationController()\n",
        "offline = OfflineFeedbackLoop()\n",
        "online = OnlineFeedbackLoop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper: Run a Single Scenario\n",
        "\n",
        "Each scenario is responsible for describing:\n",
        "\n",
        "- query\n",
        "- metadata (domain, thresholds, etc.)\n",
        "- retrieved_docs\n",
        "- reference\n",
        "- output\n",
        "\n",
        "In a real system, these would come from logs. Here we simulate them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_scenario(scenario):\n",
        "    params = scenario.parameters\n",
        "\n",
        "    query = params[\"query\"]\n",
        "    metadata = params.get(\"metadata\", {})\n",
        "    retrieved_docs = params.get(\"retrieved_docs\", [])\n",
        "    reference = params[\"reference\"]\n",
        "    output = params[\"output\"]\n",
        "\n",
        "    metrics = compute_all_metrics(\n",
        "        reference=reference,\n",
        "        output=output,\n",
        "        latency_ms=metadata.get(\"latency_ms\", 120),\n",
        "        token_count=metadata.get(\"token_count\", 80),\n",
        "        retrieval_ms=metadata.get(\"retrieval_ms\", 30),\n",
        "    )\n",
        "\n",
        "    stage_results = evaluate_all_stages(\n",
        "        query=query,\n",
        "        metadata=metadata,\n",
        "        retrieved_docs=retrieved_docs,\n",
        "        gen_metrics={\"intrinsic\": metrics.intrinsic},\n",
        "    )\n",
        "\n",
        "    action_name = controller.choose_action(stage_results)\n",
        "    correction = controller.execute(action_name)\n",
        "\n",
        "    record = {\n",
        "        \"scenario\": scenario.name,\n",
        "        \"description\": scenario.description,\n",
        "        \"metrics_intrinsic\": metrics.intrinsic,\n",
        "        \"stage_results\": stage_results,\n",
        "        \"action_name\": action_name,\n",
        "        \"correction\": correction,\n",
        "    }\n",
        "\n",
        "    offline.record(record)\n",
        "\n",
        "    if not stage_results[\"overall_passed\"]:\n",
        "        online.report_issue({\n",
        "            \"scenario\": scenario.name,\n",
        "            \"action\": action_name,\n",
        "        })\n",
        "\n",
        "    return record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run All Scenarios\n",
        "\n",
        "We now simulate the full continuous evaluation loop for each scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "results = [run_scenario(s) for s in scenarios]\n",
        "\n",
        "len(results), results[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Offline Summary\n",
        "\n",
        "Offline summary is useful for regression testing and historical analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "offline.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Online Drift Indicator\n",
        "\n",
        "Online loop only cares if the current system appears to be drifting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "online.has_drift(), online.live_issues[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, you can:\n",
        "\n",
        "- export `results` as JSON to `data/logs/sample_evaluation_runs.jsonl`\n",
        "- feed them into a dashboard (see `dashboards/examples/`)\n",
        "- refine thresholds and actions based on real behavior\n",
        "\n",
        "This completes the end-to-end simulation of the continuous evaluation\n",
        "pipeline described in the blog."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
