{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 – Multi-Stage Evaluation Loop (Stage A/B/C)\n",
        "\n",
        "This notebook operationalizes Step 2 from the blog:\n",
        "\n",
        "- **Stage A** – Pre-retrieval checks (query and routing)\n",
        "- **Stage B** – Post-retrieval checks (relevance, coverage, redundancy)\n",
        "- **Stage C** – Post-generation checks (intrinsic quality / grounding)\n",
        "\n",
        "It uses:\n",
        "\n",
        "- `rag_eval.metrics` for intrinsic/extrinsic/behavioral metrics\n",
        "- `rag_eval.stages` for stage-level evaluations\n",
        "\n",
        "The goal is to show how a vague complaint like:\n",
        "\n",
        "> \"Answers are not consistent anymore.\"\n",
        "\n",
        "can be converted into **localized failures** in a structured evaluation loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
        "\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.append(SRC_PATH)\n",
        "\n",
        "from rag_eval.metrics import compute_all_metrics\n",
        "from rag_eval.stages import (\n",
        "    stage_a_pre_retrieval,\n",
        "    stage_b_post_retrieval,\n",
        "    stage_c_post_generation,\n",
        "    evaluate_all_stages,\n",
        ")\n",
        "\n",
        "PROJECT_ROOT, SRC_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Healthy Pipeline\n",
        "\n",
        "We start with a case where everything works as expected:\n",
        "\n",
        "- Query is valid\n",
        "- Retrieval returns enough unique documents\n",
        "- Generated answer aligns well with the reference\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "query = \"What changed in the AI auditing policy in 2024?\"\n",
        "metadata = {\"domain\": \"policy\", \"max_query_tokens\": 50}\n",
        "\n",
        "retrieved_docs = [\n",
        "    {\"id\": \"doc1\", \"content\": \"Policy updated in 2024 to include AI auditing guidelines.\"},\n",
        "    {\"id\": \"doc2\", \"content\": \"Details of AI auditing process introduced in 2024.\"},\n",
        "    {\"id\": \"doc3\", \"content\": \"Background on the AI auditing requirements.\"},\n",
        "]\n",
        "\n",
        "reference = \"The policy was updated in 2024 to include new AI auditing guidelines.\"\n",
        "output = \"The policy was updated in 2024 with new guidelines for AI auditing and oversight.\"\n",
        "\n",
        "metrics = compute_all_metrics(\n",
        "    reference=reference,\n",
        "    output=output,\n",
        "    latency_ms=120,\n",
        "    token_count=90,\n",
        "    retrieval_ms=30,\n",
        ")\n",
        "\n",
        "stage_results = evaluate_all_stages(\n",
        "    query=query,\n",
        "    metadata=metadata,\n",
        "    retrieved_docs=retrieved_docs,\n",
        "    gen_metrics={\"intrinsic\": metrics.intrinsic},\n",
        ")\n",
        "\n",
        "stage_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, all stages should pass, and `overall_passed` will be `True`.\n",
        "\n",
        "The diagnostics dictionary for each stage is what we will later feed into\n",
        "controllers and feedback loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Retrieval Drift (Stage B Fails)\n",
        "\n",
        "Now we simulate a case where retrieval no longer returns enough\n",
        "distinct documents. This often appears when indexing or filtering\n",
        "changes unintentionally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "query = \"What changed in the AI auditing policy in 2024?\"\n",
        "metadata = {\"domain\": \"policy\", \"max_query_tokens\": 50}\n",
        "\n",
        "retrieved_docs_drifted = [\n",
        "    {\"id\": \"doc1\", \"content\": \"Some unrelated content.\"},\n",
        "]\n",
        "\n",
        "reference = \"The policy was updated in 2024 to include new AI auditing guidelines.\"\n",
        "output = \"The policy changed recently, but details are unclear.\"\n",
        "\n",
        "metrics_drifted = compute_all_metrics(\n",
        "    reference=reference,\n",
        "    output=output,\n",
        "    latency_ms=130,\n",
        "    token_count=70,\n",
        "    retrieval_ms=25,\n",
        ")\n",
        "\n",
        "stage_results_drifted = evaluate_all_stages(\n",
        "    query=query,\n",
        "    metadata=metadata,\n",
        "    retrieved_docs=retrieved_docs_drifted,\n",
        "    gen_metrics={\"intrinsic\": metrics_drifted.intrinsic},\n",
        ")\n",
        "\n",
        "stage_results_drifted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we should see `stage_b[\"passed\"] == False`, while `stage_a` may still pass.\n",
        "\n",
        "This is exactly the localization behavior we want: instead of “the system\n",
        "is bad,” we can say “retrieval is not returning enough quality context.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Reasoning / Grounding Degradation (Stage C Fails)\n",
        "\n",
        "Here we keep the query and retrieval healthy, but generate an answer that\n",
        "has low intrinsic overlap with the reference (simulating hallucination or\n",
        "poor grounding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "query = \"What changed in the AI auditing policy in 2024?\"\n",
        "metadata = {\"domain\": \"policy\", \"max_query_tokens\": 50}\n",
        "\n",
        "retrieved_docs_ok = retrieved_docs  # reuse from healthy example\n",
        "\n",
        "reference = \"The policy was updated in 2024 to include new AI auditing guidelines.\"\n",
        "output_bad = \"The policy introduced new marketing requirements unrelated to AI.\"\n",
        "\n",
        "metrics_bad = compute_all_metrics(\n",
        "    reference=reference,\n",
        "    output=output_bad,\n",
        "    latency_ms=115,\n",
        "    token_count=75,\n",
        "    retrieval_ms=28,\n",
        ")\n",
        "\n",
        "stage_results_bad = evaluate_all_stages(\n",
        "    query=query,\n",
        "    metadata=metadata,\n",
        "    retrieved_docs=retrieved_docs_ok,\n",
        "    gen_metrics={\"intrinsic\": metrics_bad.intrinsic},\n",
        ")\n",
        "\n",
        "stage_results_bad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we should see Stage C fail, while Stage A and B can still pass.\n",
        "\n",
        "In the next notebook, we will use these stage outputs as inputs to an\n",
        "evaluation controller that decides which corrective action to apply: fix\n",
        "the query, adjust retrieval, or adjust the prompt."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
