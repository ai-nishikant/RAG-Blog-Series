{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 – Metrics and Signals for RAG Evaluation\n",
        "\n",
        "This notebook demonstrates the three metric families from the blog:\n",
        "\n",
        "- **Intrinsic metrics** – output quality (simple lexical overlap in this demo)\n",
        "- **Extrinsic metrics** – operational behavior (latency, token usage, retrieval time)\n",
        "- **Behavioral metrics** – reasoning behavior (verbosity)\n",
        "\n",
        "It uses the functions implemented in `src/rag_eval/metrics.py`.\n",
        "\n",
        "You can extend this notebook with your own queries, reference answers,\n",
        "and simulated drift scenarios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Adjust this path if you run the notebook from a different working directory.\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
        "\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.append(SRC_PATH)\n",
        "\n",
        "from rag_eval.metrics import (\n",
        "    compute_intrinsic_metrics,\n",
        "    compute_extrinsic_metrics,\n",
        "    compute_behavioral_metrics,\n",
        "    compute_all_metrics,\n",
        ")\n",
        "from rag_eval.metrics import MetricsResult\n",
        "\n",
        "PROJECT_ROOT, SRC_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Example (Blog Snippet)\n",
        "\n",
        "We start with the same reference and output pair used in the blog to\n",
        "highlight how intrinsic, extrinsic, and behavioral metrics are computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "reference = \"The policy was updated in 2024 to include new AI auditing guidelines.\"\n",
        "output = \"The new guidelines were introduced recently for AI auditing.\"\n",
        "\n",
        "result: MetricsResult = compute_all_metrics(\n",
        "    reference=reference,\n",
        "    output=output,\n",
        "    latency_ms=120,\n",
        "    token_count=85,\n",
        "    retrieval_ms=30,\n",
        ")\n",
        "\n",
        "print(\"Intrinsic score:\", result.intrinsic)\n",
        "print(\"Extrinsic metrics:\", result.extrinsic)\n",
        "print(\"Behavioral metrics:\", result.behavioral)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Examples\n",
        "\n",
        "Now we simulate several reference/output pairs, with different levels of\n",
        "alignment and different latency/usage patterns.\n",
        "\n",
        "This mimics what you would see if you logged evaluation metrics for\n",
        "real RAG answers over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"id\": \"policy_guidelines_good\",\n",
        "        \"reference\": \"The policy was updated in 2024 to include new AI auditing guidelines.\",\n",
        "        \"output\": \"The policy was updated in 2024 with new guidelines for AI auditing.\",\n",
        "        \"latency_ms\": 110,\n",
        "        \"token_count\": 80,\n",
        "        \"retrieval_ms\": 25,\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"refund_policy_paraphrased\",\n",
        "        \"reference\": \"Refund requests must be submitted within 30 days of purchase.\",\n",
        "        \"output\": \"Customers can ask for a refund within a month of buying.\",\n",
        "        \"latency_ms\": 95,\n",
        "        \"token_count\": 64,\n",
        "        \"retrieval_ms\": 20,\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"incorrect_statement\",\n",
        "        \"reference\": \"The model does not support bulk exports in the free tier.\",\n",
        "        \"output\": \"Bulk exports are available in all subscription tiers.\",\n",
        "        \"latency_ms\": 140,\n",
        "        \"token_count\": 72,\n",
        "        \"retrieval_ms\": 35,\n",
        "    },\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for ex in examples:\n",
        "    metrics = compute_all_metrics(\n",
        "        reference=ex[\"reference\"],\n",
        "        output=ex[\"output\"],\n",
        "        latency_ms=ex[\"latency_ms\"],\n",
        "        token_count=ex[\"token_count\"],\n",
        "        retrieval_ms=ex[\"retrieval_ms\"],\n",
        "    )\n",
        "    rows.append({\n",
        "        \"id\": ex[\"id\"],\n",
        "        \"intrinsic\": metrics.intrinsic,\n",
        "        \"latency_ms\": metrics.extrinsic[\"latency_ms\"],\n",
        "        \"token_count\": metrics.extrinsic[\"token_count\"],\n",
        "        \"retrieval_ms\": metrics.extrinsic.get(\"retrieval_ms\", None),\n",
        "        \"length\": metrics.behavioral[\"length\"],\n",
        "    })\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use this table as an input into later stages:\n",
        "\n",
        "- Stage A/B/C evaluation (next notebook)\n",
        "- Drift detection over time\n",
        "- Visualization in a dashboard\n",
        "\n",
        "At this point, we have a basic but concrete representation of the\n",
        "three metric families discussed in the blog."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
